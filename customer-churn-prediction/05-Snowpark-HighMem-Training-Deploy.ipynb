{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74fa8f0e",
   "metadata": {},
   "source": [
    "# Customer churn analysis\n",
    "\n",
    "\n",
    "# Machine Learning Pipeline\n",
    "\n",
    "In the following notebooks, we will go through the implementation of each one of the steps in the Machine Learning Pipeline. \n",
    "\n",
    "We will discuss:\n",
    "\n",
    "1. Data Preparation and Analysis\n",
    "2. **Feature Engineering**\n",
    "3. **Feature Selection**\n",
    "4. **Model Training**\n",
    "5. **Obtaining Predictions / Scoring**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae7750a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from snowflake.snowpark.session import Session\n",
    "from snowflake.snowpark import functions as F\n",
    "from snowflake.snowpark.types import *\n",
    "import pandas as pd\n",
    "from sklearn import linear_model\n",
    "import matplotlib.pyplot as plt\n",
    "from snowflake.snowpark.functions import udf\n",
    "%matplotlib inline\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "# to divide train and test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# feature scaling\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# to save the trained scaler class\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9bb1d0a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 7, 0)\n",
      "[Row(CURRENT_WAREHOUSE()='LAB_L_WH_SO_HM', CURRENT_ROLE()='SYSADMIN', CURRENT_DATABASE()='DEMO', CURRENT_SCHEMA()='TELCO')]\n"
     ]
    }
   ],
   "source": [
    "#Snowflake connection info\n",
    "from config import snowflake_conn_prop\n",
    "from snowflake.snowpark import version\n",
    "print(version.VERSION)\n",
    "\n",
    "session = Session.builder.configs(snowflake_conn_prop).create()\n",
    "session.sql(\"create or replace warehouse HIGH_MEM_WH_LARGE with \\\n",
    "                WAREHOUSE_SIZE = LARGE \\\n",
    "                warehouse_type = 'HIGH_MEMORY' \\\n",
    "                AUTO_SUSPEND = 120 \\\n",
    "                AUTO_RESUME = TRUE\").collect()\n",
    "\n",
    "session.sql('Use Warehouse HIGH_MEM_WH_LARGE').collect()\n",
    "print(session.sql('select current_warehouse(),current_role(), current_database(), current_schema()').collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5be4ffa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 802 ms, sys: 84.7 ms, total: 887 ms\n",
      "Wall time: 1min 37s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "raw = session.table('LARGE_TRAINING_DATASET').sample(n = 100)\n",
    "data = raw.toPandas()\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22f4f5d",
   "metadata": {},
   "source": [
    "# Separate dataset into train and test\n",
    "\n",
    "It is important to separate our data intro training and testing set. \n",
    "\n",
    "When we engineer features, some techniques learn parameters from data. It is important to learn these parameters only from the train set. This is to avoid over-fitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d4e217",
   "metadata": {},
   "source": [
    "# Snowflake Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80374d58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(status='MODELS already exists, statement succeeded.')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create Stage\n",
    "\n",
    "query = \"create stage if not exists models\" +\\\n",
    "        \" directory = (enable = true)\" +\\\n",
    "        \" copy_options = (on_error='skip_file')\"\n",
    "        \n",
    "session.sql(query).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fb4db924",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 33.4 ms, sys: 3.19 ms, total: 36.6 ms\n",
      "Wall time: 6.78 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Create Stored Proc\n",
    "\n",
    "# setup pipeline\n",
    "\n",
    "# essential libraries\n",
    "\n",
    "\n",
    "\n",
    "from snowflake.snowpark.functions import sproc\n",
    "import snowflake.snowpark\n",
    "import json\n",
    "import cachetools\n",
    "import io\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from joblib import dump\n",
    "\n",
    "#transformations\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "#Classifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "#Pipeline\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "session.add_packages('snowflake-snowpark-python', 'scikit-learn', 'pandas', 'numpy', 'joblib', 'cachetools')\n",
    "\n",
    "def train_model(session: snowflake.snowpark.Session) -> float:\n",
    "    snowdf = session.table(\"LARGE_TRAINING_DATASET\")\n",
    "    # split the train and test set\n",
    "    snowdf_train, snowdf_test = snowdf.random_split([0.8, 0.2], seed=82) # use seed to make the split repeatable\n",
    "    \n",
    "\n",
    "    # save the train and test sets as time stamped tables in Snowflake \n",
    "    snowdf_train.write.mode(\"overwrite\").save_as_table(\"CHURN_TRAIN\")\n",
    "    snowdf_test.write.mode(\"overwrite\").save_as_table(\"CHURN_TEST\")\n",
    "    \n",
    "       \n",
    "    churn_train = snowdf_train.drop(\"CHURNVALUE\",\"CUSTOMERID\").to_pandas() # drop labels for training set\n",
    "    churn_train_labels = snowdf_train.select(\"CHURNVALUE\").to_pandas()\n",
    "    \n",
    "    churn_test = snowdf_test.drop(\"CHURNVALUE\",\"CUSTOMERID\").to_pandas()\n",
    "    churn_test_labels = snowdf_test.select(\"CHURNVALUE\").to_pandas()\n",
    "    \n",
    "    cat_vars = ['GENDER', 'SENIORCITIZEN', 'PARTNER', 'DEPENDENTS', 'PHONESERVICE', 'MULTIPLELINES', 'INTERNETSERVICE',\n",
    "            'ONLINESECURITY', 'ONLINEBACKUP', 'DEVICEPROTECTION', 'TECHSUPPORT', 'STREAMINGTV', 'STREAMINGMOVIES',\n",
    "            'CONTRACT', 'PAPERLESSBILLING', 'PAYMENTMETHOD']\n",
    "\n",
    "    # we will capture those of type numerical from previous notebook\n",
    "    num_vars = [ 'TENUREMONTHS', 'MONTHLYCHARGES', 'TOTALCHARGES']\n",
    "    \n",
    "    \n",
    "    # Model Pipeline\n",
    "    num_pipe = Pipeline([\n",
    "            ('imputer', SimpleImputer(missing_values=np.nan, strategy='constant', fill_value=0)),\n",
    "            ('scaler', MinMaxScaler()),\n",
    "        ])\n",
    "\n",
    "\n",
    "    preprocessor = ColumnTransformer([\n",
    "            ('num_transform', num_pipe, num_vars),\n",
    "            ('ordinalEncoding',OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1), cat_vars)\n",
    "        ])\n",
    "    \n",
    "    clf = make_pipeline(RandomForestClassifier(random_state=0, n_jobs=-1))\n",
    "    \n",
    "    model = Pipeline([\n",
    "            ('preprocessor', preprocessor),\n",
    "            ('model', clf),\n",
    "        ])\n",
    "\n",
    "    # fit the model\n",
    "    model.fit(churn_train, churn_train_labels)\n",
    "    \n",
    "    # save the full pipeline including the model\n",
    "    # save_file(session, model, \"@MODELS/churn_model_reg.joblib\")\n",
    "    \n",
    "    ## Upload trained model to a stage\n",
    "    \n",
    "    model_file = os.path.join('/tmp', 'churn_model_hmw.joblib')\n",
    "    dump(model, model_file)\n",
    "    session.file.put(model_file,'@MODELS',overwrite=True)\n",
    "    \n",
    "    # predict on the test set and return the root mean squared error (RMSE)\n",
    "    churn_predictions = model.predict(churn_test)\n",
    "    lin_mse = mean_squared_error(churn_test_labels, churn_predictions)\n",
    "    lin_rmse = np.sqrt(lin_mse)\n",
    "    return lin_rmse\n",
    "\n",
    "# Create an instance of StoredProcedure using the sproc() function\n",
    "train_model_sp = sproc(train_model, replace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d48c9cfc",
   "metadata": {},
   "source": [
    "## Running above cell creates a training function and initialize a stored procedure as a wrapper to run this function in Snowflake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "df962f08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6942361541431831"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "session.sql(\"alter warehouse LAB_L_WH_SO_HM set max_concurrency_level = 1\").collect()\n",
    "train_model_sp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c40f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import cachetools\n",
    "import os\n",
    "from snowflake.snowpark.functions import udf\n",
    "session.add_import(\"@MODELS/churn_model_reg.joblib\")\n",
    "\n",
    "@cachetools.cached(cache={})\n",
    "def read_file(filename):\n",
    "       import_dir = sys._xoptions.get(\"snowflake_import_directory\")\n",
    "       if import_dir:\n",
    "              with open(os.path.join(import_dir, filename), 'rb') as file:\n",
    "                     m = joblib.load(file)\n",
    "                     return m\n",
    "\n",
    "cat_vars = ['GENDER', 'SENIORCITIZEN', 'PARTNER', 'DEPENDENTS', 'PHONESERVICE', 'MULTIPLELINES', 'INTERNETSERVICE',\n",
    "            'ONLINESECURITY', 'ONLINEBACKUP', 'DEVICEPROTECTION', 'TECHSUPPORT', 'STREAMINGTV', 'STREAMINGMOVIES',\n",
    "            'CONTRACT', 'PAPERLESSBILLING', 'PAYMENTMETHOD']\n",
    "\n",
    "# we will capture those of type numerical from previous notebook\n",
    "num_vars = [ 'TENUREMONTHS', 'MONTHLYCHARGES', 'TOTALCHARGES']\n",
    "\n",
    "features = cat_vars + num_vars\n",
    "\n",
    "@udf(name='predict_churn_sp',is_permanent = True, stage_location = '@MODELSTAGE', replace=True)\n",
    "def predict_churn_sp(args: list) -> float:\n",
    "    model = read_file('churn_model_reg.joblib') \n",
    "    row = pd.DataFrame([args], columns=features)\n",
    "    return model.predict(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958c0fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = session.table('TRAIN_DATASET').sample(n = 400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324664c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "new_df.select(new_df.CUSTOMERID,new_df.CHURNVALUE, \\\n",
    "              F.call_udf(\"predict_churn_sp\", F.array_construct(*features)).alias('PREDICTED_CHURN')) \\\n",
    "        .write.mode('overwrite').saveAsTable('churn_detection_sp')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f047ac5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "session.table('churn_detection_sp').toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e18b0f",
   "metadata": {},
   "source": [
    "## if we want to do the prediction using SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df775ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "session.sql(' select customerid,churnvalue, \\\n",
    "            predict_churn_sp(ARRAY_CONSTRUCT( \\\n",
    "                                    GENDER, \\\n",
    "                                    SENIORCITIZEN, \\\n",
    "                                    PARTNER, \\\n",
    "                                    DEPENDENTS, \\\n",
    "                                    PHONESERVICE, \\\n",
    "                                    MULTIPLELINES,  \\\n",
    "                                    INTERNETSERVICE,  \\\n",
    "                                    ONLINESECURITY,  \\\n",
    "                                    ONLINEBACKUP, \\\n",
    "                                    DEVICEPROTECTION,  \\\n",
    "                                    TECHSUPPORT,  \\\n",
    "                                    STREAMINGTV,  \\\n",
    "                                    STREAMINGMOVIES, \\\n",
    "                                    CONTRACT,  \\\n",
    "                                    PAPERLESSBILLING,  \\\n",
    "                                    PAYMENTMETHOD,  \\\n",
    "                                    TENUREMONTHS, \\\n",
    "                                    MONTHLYCHARGES,  \\\n",
    "                                    TOTALCHARGES)) as Churn_prediction \\\n",
    "                                    from train_dataset sample (10 rows)').show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42769e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "session.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3def27e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "getting_started_snowpark_python",
   "language": "python",
   "name": "getting_started_snowpark_python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
